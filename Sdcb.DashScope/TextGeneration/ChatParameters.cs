using System.Text.Json.Serialization;

namespace Sdcb.DashScope.TextGeneration;

/// <summary>
/// Parameters for LLM execution.
/// </summary>
public record ChatParameters
{
    /// <summary>
    /// Format of the result - "text" for old text version, "message" for OpenAI compatible message.
    /// <para>This field must be "text" in for language model, not been used in VL model.</para>
    /// </summary>
    [JsonPropertyName("result_format")]
    public string ResultFormat { get; } = "message";

    /// <summary>
    /// Seed for the random number generator to control the randomness of the model's generation.
    /// Using the same seed allows for the reproducibility of the model's output.
    /// </summary>
    [JsonPropertyName("seed")]
    public ulong? Seed { get; set; }

    /// <summary>
    /// Limits the number of tokens to generate. The limit sets the maximum but does not guarantee
    /// that exactly that many tokens will be generated. This field is optional.
    /// </summary>
    [JsonPropertyName("max_tokens")]
    public int? MaxTokens { get; set; }

    /// <summary>
    /// The probability threshold for nucleus sampling which controls the diversity of the generated text.
    /// A higher top_p value leads to more diverse text, while a lower value results in more deterministic output.
    /// This parameter helps manage the randomness-versus-determinism balance in the text generation process.
    /// <para>Value range: (0, 1.0].</para>
    /// <para>Note: Since both temperature and top_p control the diversity of the generated text,
    /// it is recommended to adjust only one of them for desired randomness effects. For more details, see the documentation on 'Temperature' and 'top_p'.</para>
    /// <para>Default values:</para>
    /// <list type="bullet">
    /// <item>
    /// <description>qwen-max, qwen-plus, qwen-turbo, qwen-open-source series: 0.8;</description>
    /// </item>
    /// <item>
    /// <description>qwen-long: 0.8;</description>
    /// </item>
    /// <item>
    /// <description>qwen-vl series: 0.001;</description>
    /// </item>
    /// <item>
    /// <description>qwen-audio series: 0.5;</description>
    /// </item>
    /// <item>
    /// <description>qwen-math series: 1.0;</description>
    /// </item>
    /// <item>
    /// <description>qwen-coder series: 0.8.</description>
    /// </item>
    /// </list>
    /// </summary>
    [JsonPropertyName("top_p")]
    public float? TopP { get; set; }

    /// <summary>
    /// Specifies the size of the candidate set for token sampling during the generation process.
    /// This parameter defines how many of the highest probability tokens are considered in a single generation step.
    /// A larger top_k value increases randomness by expanding the set of considered tokens, whereas a smaller value increases determinism by focusing on the most likely tokens.
    /// Setting this parameter to null or greater than 100 effectively disables the top_k strategy, at which point only the top_p strategy is effective.
    /// <para>For example:</para>
    /// <list type="bullet">
    /// <item><description>If top_k is set to 50, only the top 50 tokens by score are considered for sampling.</description></item>
    /// <item><description>If top_k is null or over 100, the system disregards top_k values and relies solely on top_p for sampling diversity.</description></item>
    /// </list>
    /// </summary>
    [JsonPropertyName("top_k")]
    public int? TopK { get; set; }

    /// <summary>
    /// Controls the repetitiveness of content in the text generated by the model. This parameter manages how likely the model is to reuse tokens that have already appeared in the generation context.
    /// A positive presence_penalty value discourages repetition, helping generate more varied and creative text, suitable for creative writing or brainstorming sessions.
    /// A negative value may increase repetition, thereby enhancing consistency and frequency of terms, which can be advantageous in scenarios like technical documentation or other formal documents.
    /// <para>Value range: [-2.0, 2.0].</para>
    /// <para>Principle: If the value is positive, the model applies a penalty to tokens that have appeared in the text, decreasing their likelihood of recurring. The magnitude of the penalty is fixed regardless of how many times a token appears.</para>
    /// <para>Example:</para>
    /// <para>Prompt: 'Translate this sentence into Chinese: "This movie is good. The plot is good, the acting is good, the music is good, and overall, the whole movie is just good. It is really good, in fact. The plot is so good, and the acting is so good, and the music is so good."' </para>
    /// <para>With penalty = 2.0: '这部电影很好。剧情很棒，演技棒，音乐也非常好听，总的来说，整部电影都好得不得了。实际上它真的很优秀。剧情非常精彩，演技出色，音乐也是那么的动听。'</para>
    /// <para>With penalty = 0.0: '这部电影很好。剧情好，演技好，音乐也好，总的来说，整部电影都很好。事实上，它真的很棒。剧情非常好，演技也非常出色，音乐也同样优秀。'</para>
    /// <para>With penalty = -2.0: '这部电影很好。情节很好，演技很好，音乐也很好，总的来说，整部电影都很好。实际上，它真的很棒。情节非常好，演技也非常好，音乐也非常好。'</para>
    /// </summary>
    [JsonPropertyName("presence_penalty")]
    public float? PresencePenalty { get; set; }

    /// <summary>
    /// Controls the diversity of generated text by adjusting the model's sampling temperature. 
    /// A higher temperature results in more varied and less predictable text, while a lower temperature produces more consistent and predictable text.
    /// The temperature parameter should ideally be used exclusively, in lieu of also adjusting top_p, to avoid potential conflicts in diversity controls.
    /// <para>Value range: [0, 2).</para>
    /// <para>Default values:</para>
    /// <list type="bullet">
    /// <item><description>qwen-max series, qwen-plus series, qwen-turbo series, and qwen-open-source series: 0.7;</description></item>
    /// <item><description>qwen-long: 1.0;</description></item>
    /// <item><description>qwen-vl series: 0.01;</description></item>
    /// <item><description>qwen-audio series: 0.7;</description></item>
    /// <item><description>qwen-math series: 0;</description></item>
    /// <item><description>qwen-coder series: 0.7.</description></item>
    /// </list>
    /// For a deeper understanding on how temperature interacts with top_p to control diversity, see the detailed documentation on 'Temperature and top_p'.
    /// </summary>
    [JsonPropertyName("temperature")]
    public float? Temperature { get; set; }

    /// <summary>
    /// Specifies the content that, upon generation, should stop the model from further output.
    /// <para>This can be a string or list of strings, a single list of token IDs or a list of token ID lists.</para>
    /// <para>For example, if the stop is set as "hello", generation stops before producing "hello";</para>
    /// <para>if set as [37763, 367], generation stops before producing the token IDs equivalent to "Observation".</para>
    /// <para>
    /// Note, this field is optional and list mode does not support mixing strings and token IDs;
    /// they should all be of the same type.
    /// </para>
    /// </summary>
    [JsonPropertyName("stop")]
    public object? Stop { get; set; }

    /// <summary>
    /// Controls whether to take search results into account during generation.
    /// <para>Note: enabling search does not guarantee that search results will be used.</para>
    /// <para>
    /// If search is enabled, the model will consider the search results as part of the prompt
    /// to potentially generate text that includes the results.
    /// </para>
    /// <para>This field is optional and defaults to false.</para>
    /// </summary>
    [JsonPropertyName("enable_search")]
    public bool? EnableSearch { get; set; }

    /// <summary>
    /// Controls whether to enable incremental output mode.
    /// <para>
    /// The default value is false, meaning subsequent outputs will contain already completed segments.
    /// When set to true, incremental output mode is activated, and subsequent outputs will not contain
    /// previous segments. The full output would need to be constructed incrementally by the user.
    /// </para>
    /// This field is optional and only applicable in streaming output modes.
    /// </summary>
    [JsonPropertyName("incremental_output")]
    public bool? IncrementalOutput { get; set; }

    /// <summary>
    /// Controls whether to enhance the default token limit for input images, applicable only for specific high-resolution capable models.
    /// This is particularly useful when working with complex or detailed images requiring finer model responses.
    /// Supported models include: qwen-vl-max, qwen-vl-max-latest, qwen-vl-max-0809, qwen-vl-plus-latest, qwen-vl-plus-0809.
    /// </summary>
    [JsonPropertyName("vl_high_resolution_images")]
    public bool? VlHighResolutionImages { get; set; }

    /// <summary>
    /// Specifies the tools that the model can utilize, represented as an array of FunctionDef objects. Each tool in this array
    /// offers specific functionalities that can enhance the generation process by providing additional actions or data.
    /// Including tools in a request can be especially beneficial when complex interaction or specialized processing is required.
    /// </summary>
    [JsonPropertyName("tools")]
    public ChatTool[]? Tools { get; set; }

    /// <summary>
    /// Controls which tool the model opts to use from the specified 'tools' list. Can take one of three forms:
    /// <list type="bullet">
    /// <item>
    /// <description>"none": No tool is invoked, regardless of the context.</description>
    /// </item>
    /// <item>
    /// <description>"auto": The model determines whether to invoke a tool based on its internal logic and the input provided.</description>
    /// </item>
    /// <item>
    /// <description>Object structure e.g., { "type": "function", "function": { "name": "get_current_time" } }: Specifies the tool directly, indicating explicit use of a predefined function or operation.</description>
    /// </item>
    /// </list>
    /// </summary>
    [JsonPropertyName("tool_choice")]
    public object? ToolChoice { get; set; }
}